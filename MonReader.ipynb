{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da2c09e-6062-4c51-b0f1-49679a2075d1",
   "metadata": {},
   "source": [
    "# MonReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce79a9-4f31-4dd4-83dd-4b7303e603eb",
   "metadata": {},
   "source": [
    "### Background:\n",
    "\n",
    "Our company develops innovative Artificial Intelligence and Computer Vision solutions that revolutionize industries. Machines that can see: We pack our solutions in small yet intelligent devices that can be easily integrated to your existing data flow. Computer vision for everyone: Our devices can recognize faces, estimate age and gender, classify clothing types and colors, identify everyday objects and detect motion. Technical consultancy: We help you identify use cases of artificial intelligence and computer vision in your industry. Artificial intelligence is the technology of today, not the future.\n",
    "\n",
    "MonReader is a new mobile document digitization experience for the blind, for researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk. It is composed of a mobile app and all the user needs to do is flip pages and everything is handled by MonReader: it detects page flips from low-resolution camera preview and takes a high-resolution picture of the document, recognizing its corners and crops it accordingly, and it dewarps the cropped document to obtain a bird's eye view, sharpens the contrast between the text and the background and finally recognizes the text with formatting kept intact, being further corrected by MonReader's ML powered redactor.\n",
    "\n",
    "MonReader is a new mobile document digitalization experience for the blind, for researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk. It is composed of a mobile app and all the user needs to do is flip pages and everything is handled by MonReader: it detects page flips from low-resolution camera preview and takes a high-resolution picture of the document, recognizing its corners and crops it accordingly, and it dewarps the cropped document to obtain a bird's eye view, sharpens the contrast between the text and the background and finally recognizes the text with formatting kept intact, being further corrected by MonReader's ML powered redactor\n",
    "\n",
    "### Goal(s):\n",
    "\n",
    "Predict if the page is being flipped using a single image.\n",
    "\n",
    "### Success Metrics:\n",
    "\n",
    "Evaluate model performance based on F1 score, the higher the better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407db093-fd39-4082-b220-4c8c5a61b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da372ff3-f4a6-4d46-9481-9d3263d21c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing function\n",
    "def load_data(data_dir=\"images\", img_size=(224, 224)):\n",
    "    \"\"\"Load all images from the dataset\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in ['notflip', 'flip']:\n",
    "        class_idx = 0 if class_name == 'notflip' else 1\n",
    "        class_dir = os.path.join(data_dir, \"training\", class_name)\n",
    "        \n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(class_dir, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, img_size)\n",
    "                img = img.astype(np.float32) / 255.0\n",
    "                \n",
    "                images.append(img)\n",
    "                labels.append(class_idx)\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3982de-2a22-4d15-abe4-5d139fba95b5",
   "metadata": {},
   "source": [
    "Next let's try to build our own CNN model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29c6455-ad07-46dc-86c8-438ed640f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_cnn(num_classes=2):\n",
    "    \"\"\"Create a simple CNN using PyTorch layers\"\"\"\n",
    "    model = nn.Sequential(\n",
    "        # Convolutional layers\n",
    "        nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        \n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        \n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        \n",
    "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2, 2),\n",
    "        \n",
    "        # Flatten\n",
    "        nn.Flatten(),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        nn.Linear(256 * 14 * 14, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf073212-f679-41b1-b444-5237e47df076",
   "metadata": {},
   "source": [
    "Next, we'll use transfer learning method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668597a7-1737-4b42-9d83-1e13aaba6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning model\n",
    "def create_transfer_model(num_classes=2):\n",
    "    \"\"\"Create a transfer learning model using ResNet18\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    \n",
    "    # Freeze the base layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace the final layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f78c79-3d4c-46c8-b4df-ce07f6fceec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    \"\"\"Training Function\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Quick validation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "       \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        print(f'Epoch {epoch+1}: Accuracy: {100 * accuracy:.1f}%, F1: {f1:.3f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f17f31b-d7c3-4203-92c0-e7495447715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluation Function\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {100 * accuracy:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Not Flip', 'Flip']))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86066474-59ce-421b-93ed-c75c3b19623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 2392 images\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "X_train_full, y_train_full = load_data()\n",
    "print(f\"Loaded {len(X_train_full)} images\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    ")\n",
    "X_test, y_test = X_train_full, y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf7a5e9-b3fa-4730-9aad-275ed0f42099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "X_test_tensor = torch.FloatTensor(X_test).permute(0, 3, 1, 2)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e6371d3-cfa8-4201-8d3e-db4b98bc316f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SIMPLE CNN\n",
      "Epoch 1: Accuracy: 64.1%, F1: 0.608\n",
      "Epoch 2: Accuracy: 75.8%, F1: 0.741\n",
      "Epoch 3: Accuracy: 91.0%, F1: 0.910\n",
      "Epoch 4: Accuracy: 55.3%, F1: 0.452\n",
      "Epoch 5: Accuracy: 87.1%, F1: 0.869\n",
      "Epoch 6: Accuracy: 71.0%, F1: 0.680\n",
      "Epoch 7: Accuracy: 93.5%, F1: 0.935\n",
      "Epoch 8: Accuracy: 72.9%, F1: 0.704\n",
      "Epoch 9: Accuracy: 97.3%, F1: 0.973\n",
      "Epoch 10: Accuracy: 96.5%, F1: 0.964\n",
      "Epoch 11: Accuracy: 96.9%, F1: 0.969\n",
      "Epoch 12: Accuracy: 68.7%, F1: 0.656\n",
      "Epoch 13: Accuracy: 48.6%, F1: 0.318\n",
      "Epoch 14: Accuracy: 61.2%, F1: 0.549\n",
      "Epoch 15: Accuracy: 95.2%, F1: 0.952\n",
      "Accuracy: 94.57%\n",
      "F1 Score: 0.946\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Flip       1.00      0.90      0.94      1230\n",
      "        Flip       0.90      1.00      0.95      1162\n",
      "\n",
      "    accuracy                           0.95      2392\n",
      "   macro avg       0.95      0.95      0.95      2392\n",
      "weighted avg       0.95      0.95      0.95      2392\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1105  125]\n",
      " [   5 1157]]\n",
      "TRAINING TRANSFER LEARNING MODEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sirak\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sirak\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Accuracy: 89.4%, F1: 0.894\n",
      "Epoch 2: Accuracy: 74.3%, F1: 0.723\n",
      "Epoch 3: Accuracy: 95.8%, F1: 0.958\n",
      "Epoch 4: Accuracy: 93.3%, F1: 0.933\n",
      "Epoch 5: Accuracy: 93.9%, F1: 0.939\n",
      "Epoch 6: Accuracy: 95.8%, F1: 0.958\n",
      "Epoch 7: Accuracy: 94.2%, F1: 0.942\n",
      "Epoch 8: Accuracy: 95.6%, F1: 0.956\n",
      "Epoch 9: Accuracy: 97.1%, F1: 0.971\n",
      "Epoch 10: Accuracy: 95.0%, F1: 0.950\n",
      "Accuracy: 95.61%\n",
      "F1 Score: 0.956\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Flip       0.92      1.00      0.96      1230\n",
      "        Flip       1.00      0.91      0.95      1162\n",
      "\n",
      "    accuracy                           0.96      2392\n",
      "   macro avg       0.96      0.95      0.96      2392\n",
      "weighted avg       0.96      0.96      0.96      2392\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1229    1]\n",
      " [ 104 1058]]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "\n",
    "print(\"TRAINING SIMPLE CNN\")\n",
    "\n",
    "simple_cnn = train_model(create_simple_cnn(), train_loader, val_loader, epochs=15)\n",
    "simple_acc = evaluate_model(simple_cnn, test_loader)\n",
    "\n",
    "\n",
    "print(\"TRAINING TRANSFER LEARNING MODEL\")\n",
    "\n",
    "transfer_model = train_model(create_transfer_model(), train_loader, val_loader, epochs=10)\n",
    "transfer_acc = evaluate_model(transfer_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e734d8-21e3-4749-8405-28cb90fe5e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL COMPARISON\n",
      "Simple CNN - Accuracy: 0.95%, F1: 0.946\n",
      "Transfer Learning - Accuracy: 0.96%, F1: 0.956\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(\"MODEL COMPARISON\")\n",
    "\n",
    "# If simple_acc is still a tuple, unpack it:\n",
    "if isinstance(simple_acc, tuple):\n",
    "    simple_acc, simple_f1 = simple_acc\n",
    "if isinstance(transfer_acc, tuple):\n",
    "    transfer_acc, transfer_f1 = transfer_acc\n",
    "\n",
    "# Then run your comparison\n",
    "print(f\"Simple CNN - Accuracy: {simple_acc:.2f}%, F1: {simple_f1:.3f}\")\n",
    "print(f\"Transfer Learning - Accuracy: {transfer_acc:.2f}%, F1: {transfer_f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
